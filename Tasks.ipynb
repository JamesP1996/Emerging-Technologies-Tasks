{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Square Root of 2 Problem\n",
    "\n",
    "## Objective\n",
    "**To Create a Function that print's the square root of 2 without using in built python modules, to 100 Decimal Places**\n",
    "\n",
    "============================================================================================================================\n",
    "\n",
    "## Problems\n",
    "\n",
    "### The Square Root of 2 is a Irrational Number\n",
    "\n",
    "If we try to get the Square root of 2 as a fraction we can run into some issues.\n",
    "Let's take it to it's simplest fractional form **M/N** which when squared can equate to **m^2 = 2 x n^2**.\n",
    "**M^2** must be a even number, to ensure a even number after we do our calculations, we also must make **n^2** a even number as if we take **m^2/n^2 = 2** , so at first thought we could simplify the fraction with something like 2/12 == 1/6 and no matter how we do it, M and N will be even numbers which means we already have a simplified fraction whether it be 2/12 or 1/6 or 4/24 etc. So something must be wrong here, The issue is that we cannot take the square root of 2 as a Fraction, it is actually a Irrational Number as it cannot be written in a ratio or fraction.\n",
    "\n",
    "The Term **Reductio ad absurdum** could be used for this case, which is also known as **\"Proof by Contradiction\"** \n",
    "According to https://en.wikipedia.org/wiki/Reductio_ad_absurdum\n",
    "\n",
    "**Reductio ad absurdum** is a a type of logical argument where one assumes a claim for the sake of argument and derives an absurd or ridiculous outcome, and then concludes that the original claim must have been wrong as it led to an absurd result.\n",
    "\n",
    "#### Conclusion \n",
    "\n",
    "The square root of 2 is \"irrational\" because if it could be written as a fraction then we would have the absurd case that the fraction would have even numbers at both top and bottom and so could always be simplified leading to endlessness.\n",
    "\n",
    "You can also find that it in decimal places can go on forever. Here is a Link to a Nasa page of the Square Root of 2 to 1 Million Decimal Places. https://apod.nasa.gov/htmltest/gifcity/sqrt2.1mil\n",
    "\n",
    "\n",
    "### Python3 does not support 100 Decimal Places of Accuracy Natively\n",
    "\n",
    "Through my coding and research I had found the difficulty with precision within python's default number assignment. I could get the number to 53 decimal places with trailing 0's using a FString and formatting the result of a algorithm to .100f, the issue was that this was highly innaccurate and did not actually print the number to 100 decimal places, after some research through the Python Documentation I found out there was a hard limit on the amount of decimal place numbers we could have.\n",
    "https://docs.python.org/3/tutorial/floatingpoint.html \n",
    "\n",
    "*\"Python floats to IEEE-754 “double precision”. 754 doubles contain 53 bits of precision, so on input the computer strives to convert 0.1 to the closest fraction it can of the form J/2**N where J is an integer containing exactly 53 bits.\"*\n",
    "\n",
    "**This was the key sentence that stuck out to me and was the root of the trailing 0's issue.**\n",
    "\n",
    "#### Conclusion\n",
    "So what was the solution to overcome these limits of the IEEE-754 Double Precision standard within python that numbers would default to. I thought over various scenearios and the realization is that I could still use strings as they are not limited by the artithmitic issues that numbers are and could print as many characters as I see fit for this purpose.\n",
    "I also came to create my own algorithm of pushing the numbers to left hand side of the decimal and making them whole numbers as this would bypass the issue and then I could format the number to place the decimal point where it needed to be.\n",
    "To do this I would need to multiply the number to the base of 10 and some very long number to give it enough space to take in enough whole numbers to make up the decimal of 100 places after string formatting.\n",
    "In this case I decided on multiplying the number 2 to the base of 10 to the power of 200 as this seemed most logical and would give me the leeway needed.\n",
    "\n",
    "\n",
    "## Solution\n",
    "After doing research on the various ways of Computing The Square Root of a number through a [Wikipedia Article](https://en.wikipedia.org/wiki/Methods_of_computing_square_roots). I came to the conclusion that progammatically the Newton Method also known as the Babylonian Method or Heron's Method would be the simpliest to program and provide a very accurate result without using in built Math Libaries/Modules in Python3 such as **Math**,**Decimal** or **Fractions**.\n",
    "I originally felt that maybe digit by digit calculation could be used and I could append each number to list or a string but programmatically this looked it could be long winded and cause more confusion. There is also the Bakhshali Method where the  approximation roughly quadruples with each iteration for the Square Root of the Number and offers high accuracy to. \n",
    "\n",
    "Overall I felt you could use any Square Root algorithm and find the Square Root of 2 to a near 100% accuracy and that even the Math Libaries in various programming languages probably uses these forms of algorithms to approximate a highly accurate square root of a irrational number or normal fractionable number.\n",
    "\n",
    "I used integers in Python as they are unbounded and could reach a very high whole number value without overflowing when not using Mathamatics based modules.\n",
    "I discovered this through various stackoverflow comments and then performing research on these comments and coming to this link.\n",
    "https://mortada.net/can-integer-operations-overflow-in-python.html\n",
    "\n",
    "\n",
    "### The Final Code and Programmatic Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square Root of 2 to 100 Decimal Places : 1.4142135623730950488016887242096980785696718753769480731766797379907324784621070388503875343276415727\n"
     ]
    }
   ],
   "source": [
    "#Newton's Method (Babylonian or Hernons)\n",
    "# Returns the square root of n (This is our sqrt2 Function but can take any input by changing N).\n",
    "def squareRoot(n): \n",
    "    # We are using n itself as \n",
    "    # initial approximation (It is accurate but not fully but would be a real starting case)\n",
    "    # Setting everything to a Integer as Integers in Python are Unbounded.\n",
    "        approximation = int(n) \n",
    "        initialization = int(1.00000)\n",
    "          \n",
    "        # accuracy decides the accuracy/precision level \n",
    "        accuracy = 0.100\n",
    "        # While the Approximation - the Intialization is greater then the accuracy Floor divide the approximation by 2\n",
    "        # and set the Intitialization to the original approximization floor divided by the new approximation.\n",
    "        # We are using floor divides also known as Integer divides to keep the number within the Integer Format.\n",
    "        while(approximation - initialization > accuracy): \n",
    "              \n",
    "            approximation = (approximation + initialization) // 2\n",
    "            initialization = n // approximation \n",
    "      \n",
    "        # When the Sum of the Approximation minus the Initialization is less than \n",
    "        # the Accuracy Return the Number as a Integer\n",
    "        return int(approximation) \n",
    "\n",
    "# Set N to a high integer value where the values are pushed to the left\n",
    "n = 2 * 10 ** 200\n",
    "\n",
    "# Print the Square root of 2. fString explanation = squareRoot of 2 function floor divide(integer divide) by 10*100 \n",
    "# and then add a decimal point \".\" and get the numbers after decimal in this case our function's \n",
    "# remainder after being divided by 10 to the power of 100 to 100 decimal places \n",
    "print(f'Square Root of 2 to 100 Decimal Places : {squareRoot(n) // 10**100}.{squareRoot(n) % 10**100 :100d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "I created a square root function that would use the Newton's Method of finding a highly accurate approximation.\n",
    "For this I took an initial approximation and set it to the inputed number(*In this case \"2\"*). I set the intialization on what number could be closest to it, as 1 as this would logically be the number we would think of when we think of the square root of 2 mathmatically as a guess. I set the accuracy to 0.100 decimal places to ensure that the algorithm would run till it reached 100 decimal places. I then setup a while loop to run to while the approximation minus the initialization is greater then the accuracy we want and then redefined the variables to try get more accurate with our final approximation so in this case I set the approximation equal to the apprxomation plus the intialization floor divided by 2 and the intialization now equal to the number integer divided by the new approximation. I did it this was as it was how the Newton's algorithm would of done it mathmatically except I am merely coding up a python3 version of it. After the accuracy is greater then our sum of minusing the numbers from eachother I return the Integer value of the approximation. \n",
    "\n",
    "This number would be quite large as I multipled 2 to the base of 10 and powered it to 200 to ensure that I had enough digit leeway while not changing the value to get the integer number to 100 decimal places.\n",
    "\n",
    "Finally I created a F-String print statement which takes in the function we created twice. The First time we are integer dividing it to the base of 10 to 100 to retrive the Intitial Integer value before the decimal point which would be \"1\". The reason I did it this way rather then typing \"1\" is because I wanted it to have mathmatical merit and make the function re-useable in the future incase someone changes N to another irrational number such as PI. Finally I add a fake decimal point after the initial \"1\" we previously printed and then print out the remainder of our square root function with the same Number value (in this case 2) modulus divided by the base of 10 to the power of 100 and formatted the number/string to 100 decimals.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-squared  Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Chi-Squared Test?\n",
    "\n",
    "A Chi Squared Test is short for **\"Pearson's Chi-Squared Test\"** and is often used in statistics to determine if there is a significant difference between the expected frequencies and the observed frequiences in one or more categories. \n",
    "\n",
    "## The Collar Jobs Example\n",
    "Taken from **[Wikipedia's](https://en.wikipedia.org/wiki/Chi-squared_test) example chi-squared test for categorical data** \n",
    "\n",
    "*Suppose there is a city of 1,000,000 residents with four neighborhoods: A, B, C, and D. A random sample of 650 residents of the city is taken and their occupation is recorded as “white collar”, “blue collar”, or “no collar”. The null hypothesis is that each person’s neighborhood of residence is independent of the person’s occupational classification. The data are tabulated as follows* **Run the below code to get the table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               A   B    C   D\n",
      "White Collar  90  60  104  95\n",
      "Blue Collar   30  50   51  20\n",
      "No collar     30  40   45  35\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import scipy.stats as stats\n",
    "\n",
    "cols = ['A', 'B', 'C', 'D']\n",
    "\n",
    "# Use Panda's Dataframe Method to print the data in a table like format\n",
    "data = pandas.DataFrame(columns=cols)\n",
    "\n",
    "# Setup the Colums in the Data\n",
    "data.loc['White Collar'] = [90, 60, 104, 95]\n",
    "data.loc['Blue Collar'] = [30, 50, 51, 20]\n",
    "data.loc['No collar'] = [30, 40, 45, 35]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the example given above and apply a chi squared test to test whether area effects the numbers of each collar type. We can report an expected distributation if column does not effect the values in each row.\n",
    "In this example wikipedia states the Chi-squared value based on this table/test is approximately **24.6**. We can use a python package known as **scipy.stats** which is imported in the above code to verify if this value is correct and then use the same package to calculate the associated ***p* value**. \n",
    "\n",
    "**Run the code below which is a continuation of the code in[1] to see the Chi Squared value and associated p value calculated by scipy.stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi Squared Value\n",
      "24.6\n",
      "\n",
      "P value for effect of area on proportion of each collar:\n",
      "0.0004098425861096696\n"
     ]
    }
   ],
   "source": [
    "chi2, pValue, degreesOfFreedom, expected = stats.chi2_contingency(data)\n",
    "print('\\nChi Squared Value')\n",
    "print (round(chi2,1))\n",
    "\n",
    "print ('\\nP value for effect of area on proportion of each collar:')\n",
    "print(pValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by rounding the value to 1 decimal place the same as wikipedia's way of formatting the number, we are retrieving the same Chi Squared test value as the example they gave and thus their value is correct."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for wikipedia to get this value they took the observed values and expected values of each cell and ran a few calculations. In this instance they took the hypthoesis that by assumption of independence the example should expect the white collar workers in neighbourhood A to be 150(total workers sample in neighbourhood A) x 349(the total white collar workers sample in all neighbourhoods)divided by (650 the overall sample total) which have them a value of **80.54** which would be our expected value for Sample A. \n",
    "\n",
    "By taking this value of 80.54 and bringing it into another calculation we can achieve the chi squared value for this column, in this case\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "By continuing these form of calculations on each total column and then summing the values together we can get the chi squared value we achieved in the code above **24.6**. \n",
    "\n",
    "### The Useful extras of scipy.stats chi2 method\n",
    "Not only does scipy.stats.chi2_contingency() method provide us the overall chi Squared value and the associated p value but it also can return the Degrees of Freedom and The Expected values so that the user does not need to do these calculations himself for a full draft and explanation of the program's maths for proofing.\n",
    "\n",
    "**Below is an example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Values: \n",
      "\n",
      "[[ 80.53846154  80.53846154 107.38461538  80.53846154]\n",
      " [ 34.84615385  34.84615385  46.46153846  34.84615385]\n",
      " [ 34.61538462  34.61538462  46.15384615  34.61538462]]\n",
      "Degrees of Freedom: \n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Values: \\n\")\n",
    "print(expected)\n",
    "\n",
    "print(\"Degrees of Freedom: \\n\")\n",
    "print(degreesOfFreedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under null hypthosis according to wikipedia,this sum has an approximately chi squared distribution where the degrees of freedom = \"6\" which is the same as the value we have above during our code test. However if a test is improbably large according to the distribution, then in this example the observer could reject the null hypothesis of independence. Null hypthesis being the assumed value by the non observer.\n",
    "\n",
    "### Related Issue - Test of Homogeneity\n",
    "If we take the case where we give every resident of each of the four neighbourhoods an equal chance of being included in this sample, then each resident has the same chance of being chosen as do all residents of the same neighborhood, but residents of different neighborhoods would have different probabilities of being chosen if the four sample sizes were not proportional to the populations of the four neighbourhoods. In this case we would perform a test of \"homogenity\" rather then \"independance\". The overall question of this test would be are the proprotions of the collars in the four neighbourhoods the same, however you could perform this test the same way just with the new altered values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Chi-Squared Tests\n",
    "\n",
    "In the real world these types of tests are often used in **crytanlayis** to compare the distribution of plain text and decrypted ciphertext. The lowest value of the test being the probability of a successful decryption. This method can be generalized for many modern cryptographic problems.\n",
    "\n",
    "Chi-Squared tests can also be used in **bioinformatics** to compare the distrubtion of certain properties in genes belonging to different categories such as disease genes,essential genes and genes relating to certain chromosones , etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References / Research Materials\n",
    "\n",
    "Chi Squared Test Wikipedia Article\n",
    "https://en.wikipedia.org/w/index.php?title=Chi-squared_test&oldid=983024096\n",
    "\n",
    "Chi Squared Test PDF by Mmcmodinagar\n",
    "http://mmcmodinagar.ac.in/econtent/zoology/CHI_SQUARE_TESTS.pdf\n",
    "\n",
    "Pearson's chi square test (goodness of fit) | Probability and Statistics | Khan Academy (Youtube Video)\n",
    "https://youtu.be/2QeDRsxSF9M\n",
    "\n",
    "Statistics How To (.coms) Article on Chi Squared Tests\n",
    "https://www.statisticshowto.com/probability-and-statistics/chi-square/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard  Deviation (STDDEV.P v.s STDDEV.S)\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In programming Standard Deviation we often use $np.sqrt(np.sum((x - np.mean(x))**2)/len(x))$ in python to calculate standard deviation. Although this solution is good for a overall population, it is not efficent for samples of a population. In microsoft excel this often known as **STDDEV.P vs STDDEV.S** in STDDEV.S we would actually use $len(x)-1$ and then **STDDEV.P** follows the same format as the above code. \n",
    "\n",
    "The reason for this difference is because it is mathematically incorrect to use **STDDEV.P** for a Sample Size of the population, and it's purpose is more used if you know the absolute value of the populations.\n",
    "For Example : We would use **STDDEV.P** to \"*get the standard deviation of the total population of people who caught the covid-19 virus in the whole of Ireland's population based on age group*\" but we would use **STDDEV.S** in the circumstances of \"*get the standard deviation among 100,000 people who caught the covid-19 virus based on age group*\". \n",
    "\n",
    "Another name for this n-1 deviation is **Bessel's Correction**, generally this correction is used to approach the bias due to finite sample sizes although in some cases this may not be enough to remove the bias and it can become necessary to do more complex multi-parameter estimations.\n",
    "\n",
    "\n",
    "## Sample Deviation Data Sets in Python\n",
    "In the below sample I will show the difference in deviation values using the two methods of calculating them.\n",
    "In this sample of the population, I am measuring coronavirus cases among different age groups in a sample size of 3000 people.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCases over 3,000 People in Different Age Groups\n",
      "=========================================================\n",
      "                0-18 19-40 41-60  61+\n",
      "Posistive Cases  100   245   120  155\n",
      "\n",
      "Population Total Deviation:\n",
      "55.57\n",
      "\n",
      "Sample Deviation (Bessel's Correction)\n",
      "55.56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Population Deviation Function\n",
    "def popDeviation(x):\n",
    "    return np.sqrt(np.sum((x - np.mean(x))**2)/len(x))\n",
    "# Sample Population Deviation Function\n",
    "def samplePopDeviation(x):\n",
    "    return np.sqrt(np.sum((x - np.mean(x))**2)/len(x)-1)\n",
    "\n",
    "print(\"\\n\\tCases over 3,000 People in Different Age Groups\\n=========================================================\")\n",
    "cols = ['0-18', '19-40', '41-60', '61+']\n",
    "values = [100, 245, 120, 155]\n",
    "# Use Panda's Dataframe Method to print the data in a table like format\n",
    "data = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Setup the Column in the Data\n",
    "data.loc['Posistive Cases'] = values\n",
    "print(data)\n",
    "\n",
    "print(\"\\nPopulation Total Deviation:\")\n",
    "print(round(popDeviation(values),2))\n",
    "\n",
    "print(\"\\nSample Deviation (Bessel's Correction)\")\n",
    "print(round(samplePopDeviation(values),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the values are closely linked but a **1/100** off each other. The issue with this , is that it's not logically accurate and it is still a point off. If we were to deal with a bigger sample set, they would probably be even further apart in value. When it comes to statistics we cannot risk being innaccurate in our values or deviations as this could make a big difference in the decesions government bodies or companies decide to make. It is not logical to use a deviation method that is meant for a whole population on a sample sized set as they do not make up the over all picture and there could be a bit of coincidence involved. For example in our dataset above , we could have gotten very lucky in who we picked and the values could be completely different in another 3000 people. So in this sceneario it actually could of been better to take the whole population and use the standard deviation calculation algorithm so we get a real result that is tangible and can be acted upon. By adding a $n-1$ factor to our sample size we can get much closer to the real value if we were take the whole population of the country, It would be nearly guranteed that the whole population deviation of let's say 4 million people in this country would be closer to the value of **55.6** then **55.7** which the **Standard Deviation without Bessel's Correction** stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References / Research Materials\n",
    "\n",
    "Standard Deviation Wikipedia\n",
    "https://en.wikipedia.org/wiki/Standard_deviation\n",
    " \n",
    "Bessel's Correction Wikipedia\n",
    "https://en.wikipedia.org/wiki/Bessel%27s_correction\n",
    "\n",
    "Standard Deviation MathsIsFun\n",
    "https://www.mathsisfun.com/data/standard-deviation.html\n",
    "\n",
    "Calculate a Sample Standard Deviation\n",
    "https://www.thoughtco.com/calculate-a-sample-standard-deviation-3126345\n",
    "\n",
    "How To Calculate Standard Deviation - The Organic Chemistry Tutor\n",
    "https://www.youtube.com/watch?v=IaTFpp-uzp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
